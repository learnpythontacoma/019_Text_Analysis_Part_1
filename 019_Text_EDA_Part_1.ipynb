{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='graphics/text_eda.jpeg'>\n",
    "\n",
    "<img src='graphics/spacer.png'>\n",
    "\n",
    "<center><font style=\"font-size:40px;\">Text Exploratory Data Analysis (EDA), Part 1 </font></center>\n",
    "<center>Prepared and coded by Ben P. Meredith, Ed.D.</center>\n",
    "\n",
    "In the last meet-up, we worked on cleaning the data we scraped from Indeed. We also saved the clean text to a DataFrame. In this meet-up, we are going to use this clean data and begin doing some text analysis,\n",
    "1. Count word frequency using two different methods (not the only two, but we will cover only two)\n",
    "1. Discover and remove stop words\n",
    "1. Find and remove duplicates\n",
    "1. Plot a word cloud as an initial analysis\n",
    "\n",
    "\n",
    "\n",
    "<p> \n",
    "</p>\n",
    "\n",
    "<font style=\"font-size:24px;\">Text EDA Introduction</font>\n",
    "\n",
    "As with any other data, text data requires an EDA (Exploratory Data Analysis). Keeping in mind that we are working on a job scraper and that our goal is to find jobs that might fit our needs (plus determine the magic words that employers are using in their announcements, and which we want to use in our resumes), we will focus our EDA on those items. \n",
    "\n",
    "We will first work through different EDA scenarios, adding further information to our DataFrame in the process. At the end of our EDA, however, we will bring each of these techniques together into our program so our EDA on future job scrapings will automatically run through an EDA for us. \n",
    "\n",
    "# Load our Libraries and Data\n",
    "\n",
    "In the next block, write the code to load the csv file saved from our last meet-up without having to copy and paste it to the new folder. Put it under the variable \"df\". Then display the head and shape of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:21.954281Z",
     "start_time": "2020-06-14T15:28:21.922309Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/benmeredith/Desktop/Python Meet-up/018_Cleaning_Text/data/data_scientist_job_search.csv', \n",
    "                 index_col = 0)\n",
    "\n",
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text EDA\n",
    "## Exercise: Word Count per Advertisement\n",
    "\n",
    "Are employers mounting large or small advertisements? Determining how many words are in each advertisement will be our first task. \n",
    "\n",
    "Let's start with writing a function that counts the number of words for any given text. You will want to\n",
    ">1. iterate through each clean_text in the DataFrame\n",
    ">1. count the number of words\n",
    ">1. return that count to a column called 'word_count' in the DataFrame in each appropriate row\n",
    "\n",
    "###  `.split( )` and `len( )` method\n",
    "In the next block, using the `.split()` and `.len()` commands, write a function that counts the words in a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:21.959261Z",
     "start_time": "2020-06-14T15:28:21.956082Z"
    }
   },
   "outputs": [],
   "source": [
    "#Count TOTAL words in a document given the document text as a string and returning the count as an integer\n",
    "\n",
    "def count_words(text):\n",
    "    text =    #insure the text is a string - it may not always be a string\n",
    "    text =    # Split based upon spaces\n",
    "    count =   #the length of the string equals the count of words\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function that you just built, \n",
    "1. write out the code to iterate through the `clean_text` in your DataFrame, \n",
    "1. count the words in each observation, \n",
    "1. then place that count in a new column called `word_count`. \n",
    "1. Finally, print out the head of the DataFrame so we see the results. \n",
    "\n",
    "This will give us more prepared data to analyze. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:22.019314Z",
     "start_time": "2020-06-14T15:28:21.961210Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Word Frequency\n",
    "\n",
    "The frequency of words as they appear in a string/corpus _may_ tell us something about the importance of that word in the overall document, or advertisement in our case here. As with most of Python's methods, there are more than one way to count the frequency of words within a string (and there are more than two that we will see in a moment). \n",
    "\n",
    "### Method 1: A StackOverflow Long Function\n",
    "\n",
    "StackOverflow [https://stackoverflow.com] is one of a coder's most frequently used tools. It is a treasure trove of useful information and code. But sometimes, it can render some interesting results that need to be examined more closely. This is the case with the first example of a method to count the frequency of words in a corpus/string. \n",
    "\n",
    "Method 1 of counting the frequency of words in a corpus comes to us from StackOverflow. It was an answer that received a high score for other coder's agreeing to it. So let's look at it and see what it is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:22.026407Z",
     "start_time": "2020-06-14T15:28:22.022047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count the FREQUENCY of words in a document text given as a string and returning the frequency\n",
    "\n",
    "def count_word_frequency(text):\n",
    "    import re # we will need to regex library to find words alone\n",
    "\n",
    "    frequency = {} #establish a dictionary to store the word as key and count as value\n",
    "\n",
    "    text_string = str(text).lower()#ensure the string to count is in fact a string and lower case\n",
    "    match_pattern = re.findall(r'\\b[a-z]{1,15}\\b', text_string)# regex to find words alone and store it in match_pattern\n",
    "\n",
    "    for word in match_pattern:# loop to count each word\n",
    "        count = frequency.get(word,0)#use the frequency dictionary to capture get each word and its count\n",
    "        frequency[word] = count + 1# adds 1 to the count because computer start numbering at zero\n",
    "\n",
    "    frequency = sorted(frequency.items(), key=lambda item: item[1], reverse=True)#sort from most to least frequent\n",
    "\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: `Counter` function\n",
    "\n",
    "Impressive as Method 1 is with all that it does, let's take a look at a second option - one that I wrote as a counter-example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:22.031535Z",
     "start_time": "2020-06-14T15:28:22.027838Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_frequency_of_words(text):\n",
    "    from collections import Counter # import the Counter function from the collections library\n",
    "    text = str(text) #force python to recognize the text as a string in case it isn't\n",
    "    word_frequency = Counter(text.split(' '))# use the split command to split by spaces\n",
    "    word_frequency = sorted(word_frequency.items(), key=lambda item: item[1], reverse=True)# Sort from most to least frequent\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Count Word Frequency in each Advertisement using Method 1\n",
    "\n",
    "Let's now use Method 1 to count the word frequency in each advertisement and save the results to our DataFrame for each advertisement. Then we will look at a single observation to get an idea of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:57:18.041077Z",
     "start_time": "2020-06-14T15:57:18.038490Z"
    }
   },
   "outputs": [],
   "source": [
    "#Method 1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:22.293208Z",
     "start_time": "2020-06-14T15:28:22.282924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's check an entry to see what it looks like\n",
    "\n",
    "df.loc[42, 'word_count_frequency']    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Count Word Frequency in each Advertisement using Method 2\n",
    "\n",
    "Now let's use Method 2 to count the word frequency in each advertisement and save the results to the DataFrame. Then we will print out the results of the same observation we just used so we can compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:22.550703Z",
     "start_time": "2020-06-14T15:28:22.295727Z"
    }
   },
   "outputs": [],
   "source": [
    "#Method 2 for counting word frequency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:22.566514Z",
     "start_time": "2020-06-14T15:28:22.552774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's check an entry to see what it looks like\n",
    "\n",
    "df.loc[42, 'word_count_frequency']    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Count frequency of all words in the dataset using Method 1\n",
    "\n",
    "In the prior two examinations, we looked at the frequency of words in each advertisement. But what are the most frequent words used in all advertisements? Knowing the most frequently used words used in our job search can help us to develop a better resume (one that passes the initial sorting algorithyms and gets our resume into the hands of a human). Knowing the most frequently used words for our search position can also help us identify the key skills for which employers (in general) are seeking in candidates. \n",
    "\n",
    "Let's use both methods again to compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:22.626915Z",
     "start_time": "2020-06-14T15:28:22.568286Z"
    }
   },
   "outputs": [],
   "source": [
    "#Method 1\n",
    "\n",
    "#Establish an empty string variable to hold all of the text\n",
    "\n",
    "#grab the text in each advertisement\n",
    "\n",
    "# make each text a string (just an insurance policy)\n",
    "\n",
    "#Concatenate each string into one long string\n",
    "\n",
    "#Use Method 1 to count word frequency\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Count frequency of all words in the dataset using Method 2\n",
    "\n",
    "Now let's use Method 2 to do the same thing and compare our results. Once again, we will see that the results are slightly different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:22.654474Z",
     "start_time": "2020-06-14T15:28:22.628799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Method 2\n",
    "\n",
    "#Establish an empty string variable to hold all of the text\n",
    "\n",
    "#grab the text in each advertisement\n",
    "\n",
    "# make each text a string (just an insurance policy)\n",
    "\n",
    "#Concatenate each string into one long string\n",
    "\n",
    "#Use Method 2 to count word frequency\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words\n",
    "\n",
    "If you take a look at our word count frequency, what words do you notice are the most frequent? \"a\", \"and\", \"to\", \"in\", and so forth. The little prepositions and conjunctions that are common in English are not in what we are interested. These small words, which do not add anything, are called, \"Stop Words\" in natural language processing. \n",
    "\n",
    "I intentionally did not talk about Stop Words before this, just so we could see how they clutter and dirty our data. While we will be getting rid of Stop Words in the next few blocks, we would normally get rid of them once we cleaned the data - in some, but not all cases. \n",
    "\n",
    "After looking at the list of stopwords that I have in the next block of code, can we think of cases where we would NOT want to remove stopwords?\n",
    "\n",
    ">__Note__: NLTK has an internal library of stopwords, which we did not talk about. While it is convenient to simply import that library's stopwords, it is not a large or complete library. The method we are using here allows you to determine what stopwords you want to use in each case. For example, as we are working with job advertisements, there are a plethora of cliche words that we may find which do not add anything to the value of the advertisement that we want to eliminate in our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:22.662609Z",
     "start_time": "2020-06-14T15:28:22.656286Z"
    }
   },
   "outputs": [],
   "source": [
    "#Remove Stopwords given the document text as a string and returning filtered text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopword_list = ('to', \n",
    "                     'a', \n",
    "                     'and', \n",
    "                     'an', \n",
    "                     'the', \n",
    "                     'of', \n",
    "                     'in', \n",
    "                     'with', \n",
    "                     'or', \n",
    "                     'at', \n",
    "                     'or', \n",
    "                     'is', \n",
    "                     'p',\n",
    "                     'for',\n",
    "                     '-',\n",
    "                    )\n",
    "    tokens = text.split(' ')#Tokenizing the words\n",
    "    tokens = [token.strip() for token in tokens]#Stripping out leading and trailing whitespace from each word\n",
    "    tokens = [str(token).lower() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    \n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Removing Stop Words and Storing New Text\n",
    "\n",
    "In the next block, using the `remove_stopwords` function that we just built, \n",
    "1. Iterate through the entire DataFrame's `clean_text`\n",
    "1. Remove the stop words\n",
    "1. Store the text in a new column called `no_stop_text`\n",
    "1. display the DataFrame head to see our work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:22.887404Z",
     "start_time": "2020-06-14T15:28:22.664382Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:22.893067Z",
     "start_time": "2020-06-14T15:28:22.888985Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[42, 'no_stop_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Word Count Frequency without Stop Words\n",
    "\n",
    "Let's run our word_count_frequency function again, but this time, let's use the text without stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:23.130547Z",
     "start_time": "2020-06-14T15:28:22.894573Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and Remove Duplicate Job Announcements\n",
    "\n",
    "Duplicate data will throw off results of analyses. Having more than one copy of the same job announcement will throw off not only our analysis, but it will throw off our applications for jobs. So let's tell Python to look at every entry (row) and eliminate duplicates. \n",
    "\n",
    "There are several ways to do this, but the simplest (and fastest) is to use the `.drop_duplicates()` function built into Pandas. As we see in Line 2 below, we are setting our parameters to look for the same `job_id` and when Python finds two duplicate `job_id`s, to keep the first one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:23.134750Z",
     "start_time": "2020-06-14T15:28:23.131898Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    df = df.drop_duplicates(subset='job_id', keep='first') \n",
    "    df = df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Remove Duplicates\n",
    "\n",
    "In the next block, write out the code to use the `remove_duplicates(df)` function above, then display the tail of the DataFrame to see our work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:23.245194Z",
     "start_time": "2020-06-14T15:28:23.136294Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:23.249964Z",
     "start_time": "2020-06-14T15:28:23.246582Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud EDA \n",
    "\n",
    "### On no_stop_text\n",
    "\n",
    "Now that we have duplicate entries removed, we have stopwords removed from our cleaned text, AND we have all of this stored in our dataset, we are ready to do some analysis of the text. \n",
    "\n",
    "Just to compare the cleaned text and the text with stopwords removed - we want to see if there is a visual difference in the text's show of word counts - let's make a WordCloud for four entries looking at the clean text and at the text without stopwords. Do we see a difference in the meanings that come out of the two texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:23.256141Z",
     "start_time": "2020-06-14T15:28:23.251753Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "stopwords = set(STOPWORDS)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:23.266066Z",
     "start_time": "2020-06-14T15:28:23.257570Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_wordcloud(data, title = None):\n",
    "    from wordcloud import WordCloud, STOPWORDS\n",
    "    import matplotlib.pyplot as plt\n",
    "    stopwords = set(STOPWORDS)\n",
    "    %matplotlib inline\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=100,\n",
    "        max_font_size=50, \n",
    "        scale=12,\n",
    "        random_state=42\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:27.499280Z",
     "start_time": "2020-06-14T15:28:23.267689Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index in range(30, 34): # just pulling a random four announcements as an example\n",
    "    print(df.loc[index, 'title'], df.loc[index, 'company'])\n",
    "    show_wordcloud(df.loc[index, 'no_stop_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And on the clean text of the same job announcements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:31.609977Z",
     "start_time": "2020-06-14T15:28:27.500724Z"
    }
   },
   "outputs": [],
   "source": [
    "for index in range(30, 34):# pulling four random samples as an example\n",
    "    print(df.loc[index, 'title'], df.loc[index, 'company'])\n",
    "    show_wordcloud(df.loc[index, 'clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Saving Our Results\n",
    "\n",
    "Let's write out the code to save our results for the next meet-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T15:28:34.809497Z",
     "start_time": "2020-06-14T15:28:34.754694Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "468px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
